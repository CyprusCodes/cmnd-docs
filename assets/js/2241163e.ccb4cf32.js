"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2099],{6985:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>a,contentTitle:()=>s,default:()=>p,frontMatter:()=>o,metadata:()=>l,toc:()=>d});var i=n(4848),t=n(8453);const o={id:"llm-proxy",title:"LLM Proxy",sidebar_position:1},s="LLM Proxy",l={id:"Open-Source & Developer Contributions/llm-proxy",title:"LLM Proxy",description:"llm-proxy is an open-source abstraction layer for large language models (LLMs) like OpenAI, Anthropic, and others. It provides a clean, unified interface for sending and receiving LLM requests \u2014 making it easier to build flexible, provider-agnostic AI features in your apps.",source:"@site/docs/Open-Source & Developer Contributions/llm-proxy.md",sourceDirName:"Open-Source & Developer Contributions",slug:"/Open-Source & Developer Contributions/llm-proxy",permalink:"/docs/Open-Source & Developer Contributions/llm-proxy",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{id:"llm-proxy",title:"LLM Proxy",sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Overview",permalink:"/docs/Open-Source & Developer Contributions/overview"},next:{title:"Community & Support",permalink:"/docs/community-and-support"}},a={},d=[{value:"How it works",id:"how-it-works",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Middleware Layer",id:"middleware-layer",level:3},{value:"Services Layer",id:"services-layer",level:3},{value:"Get Involved",id:"get-involved",level:2}];function c(e){const r={a:"a",admonition:"admonition",br:"br",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.header,{children:(0,i.jsx)(r.h1,{id:"llm-proxy",children:"LLM Proxy"})}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"llm-proxy"})," is an open-source abstraction layer for large language models (LLMs) like OpenAI, Anthropic, and others. It provides a clean, unified interface for sending and receiving LLM requests \u2014 making it easier to build flexible, provider-agnostic AI features in your apps."]}),"\n",(0,i.jsx)(r.p,{children:"We built it to simplify the overhead of managing multiple providers, normalize their differences, and support dynamic routing logic in real production systems."}),"\n",(0,i.jsx)(r.p,{children:"Whether you're experimenting or shipping, LLM Proxy helps you focus on your product \u2014 not your LLM wiring."}),"\n",(0,i.jsx)(r.admonition,{type:"info",children:(0,i.jsxs)(r.p,{children:["\ud83d\udce6 ",(0,i.jsx)(r.a,{href:"https://www.npmjs.com/package/llm-proxy",children:"npm Package"})]})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"how-it-works",children:"How it works"}),"\n",(0,i.jsx)(r.p,{children:"LLM Proxy sits between your application and the LLM providers you use. From a single entrypoint, it figures out which provider to use, adapts the input format to what that provider expects, executes the request, and returns a clean, consistent response."}),"\n",(0,i.jsx)(r.p,{children:"This makes it easy to:"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"Swap providers (or add fallbacks) without rewriting logic"}),"\n",(0,i.jsx)(r.li,{children:"Streamline request/response handling"}),"\n",(0,i.jsx)(r.li,{children:"Keep your app logic simple and decoupled from LLM-specific quirks"}),"\n"]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.img,{alt:"LLM Proxy Flow",src:n(7504).A+"",width:"1034",height:"530"})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"architecture",children:"Architecture"}),"\n",(0,i.jsx)(r.p,{children:"LLM Proxy is structured into two core layers:"}),"\n",(0,i.jsx)(r.h3,{id:"middleware-layer",children:"Middleware Layer"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Provider Resolver"}),(0,i.jsx)(r.br,{}),"\n","Routes the request to the appropriate LLM provider, based on runtime config, user input, or defaults."]}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Input Adapter"}),(0,i.jsx)(r.br,{}),"\n","Maps app-level input into the format expected by the target provider."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(r.h3,{id:"services-layer",children:"Services Layer"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Provider Client"}),(0,i.jsx)(r.br,{}),"\n","Executes the actual API call (e.g., to OpenAI or Anthropic) and handles any errors or retries."]}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Output Adapter"}),(0,i.jsx)(r.br,{}),"\n","Transforms the response back into a normalized, provider-agnostic format for your app."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.img,{alt:"LLM Proxy System Diagram",src:n(2332).A+"",width:"800",height:"366"})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"get-involved",children:"Get Involved"}),"\n",(0,i.jsx)(r.p,{children:"We believe great developer tools should be open \u2014 and built with developers in mind."}),"\n",(0,i.jsx)(r.p,{children:"LLM Proxy is open source, extensible, and ready for your ideas. If you want to add new providers, improve the internals, or help shape its future \u2014 we\u2019d love to have you involved."}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Github:"})," ",(0,i.jsx)(r.strong,{children:(0,i.jsx)(r.a,{href:"https://github.com/CyprusCodes/llm-proxy",children:"https://www.github.com/CyprusCodes/llm-proxy"})})]}),"\n",(0,i.jsxs)(r.p,{children:["Explore the code, open an issue, submit a PR, or just ",(0,i.jsx)(r.a,{href:"mailto:info@cmnd.ai",children:"reach out"})," :)"]}),"\n",(0,i.jsx)(r.p,{children:"Let\u2019s make LLM infrastructure better \u2014 together."})]})}function p(e={}){const{wrapper:r}={...(0,t.R)(),...e.components};return r?(0,i.jsx)(r,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},7504:(e,r,n)=>{n.d(r,{A:()=>i});const i=n.p+"assets/images/llm-proxy-flow-a5d784b232d0918afd28664adf96671a.gif"},2332:(e,r,n)=>{n.d(r,{A:()=>i});const i=n.p+"assets/images/llm-proxy-973a6b56f76d8c83a27bcabbec9ff53c.jpeg"},8453:(e,r,n)=>{n.d(r,{R:()=>s,x:()=>l});var i=n(6540);const t={},o=i.createContext(t);function s(e){const r=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function l(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(o.Provider,{value:r},e.children)}}}]);